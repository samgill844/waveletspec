#!/usr/bin/python



import argparse
import waveletspec as w
import numpy as np
from astropy.io import fits
import emcee
import corner
from progress.bar import Bar
################################################################################################################
# SECTION 0 - IMPORTS 
################################################################################################################
import logging
import multiprocessing as mp
from multiprocessing import Pool
import mlpy.wavelet as w
from scipy.interpolate import interp1d
import glob
LOG_LEVEL = "critical"
logger = logging.getLogger() # root logger, common for all
logger.setLevel(logging.getLevelName(LOG_LEVEL.upper()))
from heapq import nsmallest
import time
import scipy.stats as s
from scipy.misc import logsumexp
import math
from scipy.stats import norm
from scipy.signal import fftconvolve
from multiprocessing import Process
from astropy.stats import mad_std
import matplotlib
from astropy.table import hstack
from matplotlib.patches import Rectangle
import subprocess
from astropy.table import Table, Column
import itertools
from progress.bar import Bar
from scipy import signal
import matplotlib.pyplot as plt

# Set up command line switched
parser = argparse.ArgumentParser(
formatter_class=argparse.ArgumentDefaultsHelpFormatter,
description='Spectrum analyis'
)


parser.add_argument("spec_file", help='Spectrum file')

parser.add_argument("-t", "--threads", default=4, type=int, help='Number of threads for emcee')

parser.add_argument("-s", "--steps", default=1024, type=int,help='Number of emcee chain steps')

parser.add_argument("-n", "--normalise_spectrum", default='False', help='Normalise the spectra or not')

parser.add_argument("-c", "--chain_file", default='chain.fits', help='Output file for chain data')

parser.add_argument("-w", "--walkers", default=10, type=int,help='Number of emcee chain')




# Get command line options
args = parser.parse_args()








#####################################
# Funtions
####################################

from astropy.io import fits


import waveletspec,os,sys
this_dir, this_filename = os.path.split(waveletspec.__file__)
print('ISPEC_1: {}/iSpec_v20160930_py2'.format(this_dir))
# First search for the iSpec tar (eithe Python2 or 3 version)
try:
    if (sys.version_info < (3, 0)):
        os.stat(this_dir+'/iSpec_v20160930_py2')
        print('Found python 2 version of iSpec')
        ispec_dir = this_dir+'/iSpec_v20160930_py2'
        sys.path.insert(0, os.path.abspath((this_dir+'/iSpec_v20160930_py2')))
    if (sys.version_info > (3, 0)):
        os.stat(this_dir+'/iSpec_v20160930_py3')
        ispec_dir = this_dir+'/iSpec_v20160930_py3'
        print('Found python 3 version of iSpec')
        sys.path.insert(0, os.path.abspath((this_dir+'/iSpec_v20160930_py3')))
except OSError as e:
    # export the tar
    print('I cant find the iSpec directory, searching for the tar files...')
    if (sys.version_info > (3, 0)):
        os.system('tar -xf {}/iSpec_v20160930_py3.tar.gz -C {}'.format(this_dir,this_dir))
        print('Exctracted to {}/iSpec_v20160930_py3'.format(this_dir))
        ispec_dir = this_dir+'/iSpec_v20160930_py3'
        sys.path.insert(0, os.path.abspath((this_dir+'/iSpec_v20160930_py3')))
        
    elif (sys.version_info < (3, 0)):
        os.system('tar -xf {}/iSpec_v20160930_py2.tar.gz -C {}'.format(this_dir,this_dir))
        print('Exctracted to {}/iSpec_v20160930_py2'.format(this_dir))
        sys.path.insert(0, os.path.abspath((this_dir+'/iSpec_v20160930_py2')))
        ispec_dir = this_dir+'/iSpec_v20160930_py2'
    else:
        print('I CANT FIND ISPEC')
import ispec

def load_spectra(name_of_file, rv_correct=False,velocity_step=1.0):
	data = 0
	try:
		data = ispec.read_spectrum(name_of_file) # using ispec
	except:
		print('File: '+name_of_file+' cannot be opend by iSpec')
		print('This may be corrept or not in the right form.')
		return
	if min(data['waveobs'])>1000:
		data['waveobs'] = data['waveobs']/10 # convert to nm

	if rv_correct == True:
		rv, rv_err = determine_radial_velocity_with_template(data,velocity_step)
		print('{}: {} +- {}'.format(name_of_file,rv,rv_err))
		data = ispec.correct_velocity(data, rv)

	data['err'] = [0.01]*len(data)

	return data



def normalise_spectra(data,resolution=55000):
	model = "Splines" # "Polynomy"
	degree = 2
	nknots = None # Automatic: 1 spline every 5 nm

	# Strategy: Filter first median values and secondly MAXIMUMs in order to find the continuum
	order='median+max'
	median_wave_range=0.05
	max_wave_range=1.0

	strong_lines = ispec.read_line_regions(ispec_dir + "/input/regions/strong_lines/absorption_lines.txt")
	star_continuum_model = ispec.fit_continuum(data, from_resolution=resolution, \
		                ignore=strong_lines, \
		                nknots=nknots, degree=degree, \
		                median_wave_range=median_wave_range, \
		                max_wave_range=max_wave_range, \
		                model=model, order=order, \
		                automatic_strong_line_detection=True, \
		                strong_line_probability=0.5, \
		                use_errors_for_fitting=True)

	data = ispec.normalize_spectrum(data, star_continuum_model, consider_continuum_errors=False)
	return data











bar_vals = {}
class Progressbar(Bar):
    suffix = '%(parameters)s %(timeleft)s  %(index)d / %(max)d'	
    @property
    def parameters(self):
	    t,m,l,v,j = np.mean(bar_vals['step'],axis=0)
	    dt,dm,dl,dv,dj = np.std(bar_vals['step'],axis=0)
	    return '{:.0f} +- {:.0f}    {:.1f} +- {:.1f}    {:.1f} +- {:.1f}     {:.1f} +- {:.1f}   '.format(t,dt,m,dm,l,dl,v,dv)

    @property
    def timeleft(self):
        return bar_vals['timeleft']




def broaden_all_fast(wavelength,flux,vmac=2,vsini=2,resolution=55000):
    #####################################
    # convert to a uniform velocity space
    #####################################
    tic_b_ = time.time()
    lnwave_ = np.linspace(np.log(wavelength[0]),np.log(wavelength[-1]),len(wavelength))
    flnw_ = np.interp(lnwave_,np.log(wavelength),flux) # Flux on uniform log wave 
    c_kms =  299792.458
    velocity_step = (np.exp(lnwave_[1]-lnwave_[0])-1)*c_kms

    # define the kernals
    #vmac_kernel,vsini_kernel,resolution_kernel = [],[],[]
    #print('\t- Logspace: {}'.format(time.time()-	tic_b_))
    #tic_b = time.time()


    kernals_to_use=[]

    if (vmac <=0) and (vsini <= 0) and (resolution <= 0):
        #print('No kernals used')
        return wavelength,flux

	#########################
	# Construct vmac kernel
	#########################
    if vmac > 0:
        # mu represent angles that divide the star into equal area annuli,
        # ordered from disk center (mu=1) to the limb (mu=0).
        # But since we don't have intensity profiles at various viewing (mu) angles
        # at this point, we just take a middle point:
        m = 0.5
        # Calc projected simga for radial and tangential velocity distributions.
        sigma = vmac/np.sqrt(2.0) / velocity_step
        sigr = sigma * m
        sigt = sigma * np.sqrt(1.0 - m**2.)
        # Figure out how many points to use in macroturbulence kernel
        nmk = max(min(round(sigma*10), (len(flux)-3)/2), 3)
        # Construct radial macroturbulence kernel w/ sigma of mu*vmac/sqrt(2)
        if sigr > 0:
            xarg = (np.arange(2*nmk+1)-nmk) / sigr   # exponential arg
            #mrkern = np.exp(max((-0.5*(xarg**2)),-20.0))
            mrkern = np.exp(-0.5*(xarg**2))
            mrkern = mrkern/mrkern.sum()
        else:
            mrkern = np.zeros(2*nmk+1)
            mrkern[nmk] = 1.0    #delta function

        # Construct tangential kernel w/ sigma of sqrt(1-mu**2)*vmac/sqrt(2.)
        if sigt > 0:
            xarg = (np.arange(2*nmk+1)-nmk) /sigt
            mtkern = np.exp(-0.5*(xarg**2))
            mtkern = mtkern/mtkern.sum()
        else:
            mtkern = np.zeros(2*nmk+1)
            mtkern[nmk] = 1.0
        ## Sum the radial and tangential components, weighted by surface area
        area_r = 0.5
        area_t = 0.5
        vmac_kernel = area_r*mrkern + area_t*mtkern
        kernals_to_use.append(vmac_kernel)
    else:
	    pass

    #print('\t- Vmac: {}'.format(time.time()-tic_b))
    #tic_b = time.time()
    #########################
    # Construct vsini kernel
    #########################

    if vsini > 0:
        epsilon=0.6
        #wave_ = np.log(wavelength)
        #velo_ = np.linspace(wave_[0],wave_[-1],len(wave_))
        #flux_ = np.interp(lnwave_,wave_,flux)
        #dvelo = velo_[1]-velo_[0]
        dvelo = velocity_step/c_kms
        vrot = vsini/(299792458.*1e-3) # in Km/s
        #-- compute the convolution kernel and normalise it
        # PFLM/SG - changed calculation of kernel to use an odd number of pixels so that 
        # there is at least one pixel in the kernel - more reliable for low vrot
        n = int(1+2*vrot/dvelo) # always have at least one pixel in the kernel !
        #n = int(3+2*vrot/dvelo) # tried at least 3 elements - it fails
        velo_k = np.arange(0,n)*dvelo
        velo_k -= velo_k[-1]/2.
        y = 1 - (velo_k/vrot)**2 # transformation of velocity
        vsini_kernel = (2*(1-epsilon)*np.sqrt(y)+np.pi*epsilon/2.*y)/(np.pi*vrot*(1-epsilon/3.0))  # the kernel
        vsini_kernel /= vsini_kernel.sum()
        kernals_to_use.append(vsini_kernel)
    else:
	    pass
	
    #vsini_kernel = [1]
    #print('\t- Vsini: {}'.format(time.time()-tic_b))
    #tic_b = time.time()
    ##############################
    # Construct resolution kernel
    ##############################
    if resolution > 0:
	    #lnwave_ = np.linspace(np.log(wavelength[0]),np.log(wavelength[-1]),len(wavelength))
	    #flnw_ = np.interp(lnwave_,np.log(wavelength),flux_spec) # Flux on uniform log wave 
	    #dvelo = (np.exp(lnwave_[1]-lnwave_[0])-1)*c_kms
	    v_fwhm =c_kms/resolution # in km/s
	    n = int(1+4*v_fwhm/velocity_step) # always have at least one pixel in the kernel !
	    velo_k = np.linspace(-n*velocity_step,n*velocity_step,2*n + 1)
	    v_sigma = v_fwhm/(2*np.sqrt(2*np.log(2)))
	    resolution_kernel =  np.exp(-0.5*velo_k**2/(v_sigma**2)) # the kernel
	    resolution_kernel /= resolution_kernel.sum()
	    kernals_to_use.append(resolution_kernel)

    else:
	    pass
    #print('\t- Resolution: {}'.format(time.time()-tic_b))
    #tic_b = time.time()

    #OLD WAY
    '''
    Interpolation: 0.0085289478302
	    - Logspace: 0.00404000282288
	    - Vmac: 0.000119924545288
	    - Vsini: 6.89029693604e-05
	    - Resolution: 7.10487365723e-05
	    - kernel combining: 0.000430107116699
	    - Actual broadening: 0.0108051300049
	    - Interpolating: 0.00414109230042
    Broadening: 0.0197830200195
    Re-interplating: 0.00380778312683
    Dicting: 0.0040590763092
    Total: 0.0362620353699


    '''
    #print('vmac kernel')
    #print(vmac_kernel)

    #print('vsini kernel')
    #print(vsini_kernel)

    #print('resolution kernel')
    #print(resolution_kernel)

    ##########################
    # Create the master kernel
    ##########################
    #print('Number of kernels: {}'.format(len(kernals_to_use)))
    #print(kernals_to_use)

    kernals_to_use = sorted(kernals_to_use, key=len)
    kernals_to_use = kernals_to_use[::-1] # list of kernels - longest first

    master_kernel = kernals_to_use[0]
    for i in range(1,len(kernals_to_use)):
        master_kernel = signal.convolve(master_kernel,kernals_to_use[i],mode='same')

    #master_kernel = signal.convolve(vmac_kernel,vsini_kernel,mode='same')
    #master_kernel = fftconvolve(vsini_kernel,vmac_kernel,mode='same')
    #print(master_kernel)
    #master_kernel = signal.convolve(master_kernel,resolution_kernel,mode='same')
    #master_kernel = fftconvolve(master_kernel,resolution_kernel,mode='same')




    #print(master_kernel)
    #print('\t- kernel combining: {}'.format(time.time()-tic_b))
    #tic_b = time.time()
    ###############################
    # Now do the final convolution
    ###############################
    #flnw_conv = 1 - signal.convolve(1-flnw_, master_kernel, mode='same') # Fastest
    flnw_conv = 1 - fftconvolve(1-flnw_, master_kernel, mode='same') # Fastest
    #print('\t- Actual broadening: {}'.format(time.time()-tic_b))
    tic_b = time.time()
    flux_conv = np.interp(wavelength,np.exp(lnwave_),flnw_conv)
    #print('\t- Interpolating: {}'.format(time.time()-tic_b))
    return wavelength, flux_conv

def broaden_all_fastn(wavelength,flux,vmac=2,vsini=2,resolution=55000):
    #####################################
    # convert to a uniform velocity space
    #####################################
    tic_b_ = time.time()
    lnwave_ = np.linspace(np.log(wavelength[0]),np.log(wavelength[-1]),len(wavelength))
    flnw_ = np.interp(lnwave_,np.log(wavelength),flux) # Flux on uniform log wave 
    c_kms =  299792.458
    velocity_step = (np.exp(lnwave_[1]-lnwave_[0])-1)*c_kms

    # define the kernals
    vmac_kernel,vsini_kernel,resolution_kernel = [],[],[]
    #print('\t- Logspace: {}'.format(time.time()-	tic_b_))
    tic_b = time.time()


    kernals_to_use=[]

    if (vmac <=0) and (vsini <= 0) and (resolution <= 0):
	    print('No kernals used')
	    return wavelength,flux

	#########################
	# Construct vmac kernel
	#########################
    if vmac > 0:
	    # mu represent angles that divide the star into equal area annuli,
	    # ordered from disk center (mu=1) to the limb (mu=0).
	    # But since we don't have intensity profiles at various viewing (mu) angles
	    # at this point, we just take a middle point:
	    m = 0.5
	    # Calc projected simga for radial and tangential velocity distributions.
	    sigma = vmac/np.sqrt(2.0) / velocity_step
	    sigr = sigma * m
	    sigt = sigma * np.sqrt(1.0 - m**2.)
	    # Figure out how many points to use in macroturbulence kernel
	    nmk = max(min(round(sigma*10), (len(flux)-3)/2), 3)
	    # Construct radial macroturbulence kernel w/ sigma of mu*vmac/sqrt(2)
	    if sigr > 0:
	        xarg = (np.arange(2*nmk+1)-nmk) / sigr   # exponential arg
	        #mrkern = np.exp(max((-0.5*(xarg**2)),-20.0))
	        mrkern = np.exp(-0.5*(xarg**2))
	        mrkern = mrkern/mrkern.sum()
	    else:
	        mrkern = np.zeros(2*nmk+1)
	        mrkern[nmk] = 1.0    #delta function

	    # Construct tangential kernel w/ sigma of sqrt(1-mu**2)*vmac/sqrt(2.)
	    if sigt > 0:
	        xarg = (np.arange(2*nmk+1)-nmk) /sigt
	        mtkern = np.exp(-0.5*(xarg**2))
	        mtkern = mtkern/mtkern.sum()
	    else:
	        mtkern = np.zeros(2*nmk+1)
	        mtkern[nmk] = 1.0

	    ## Sum the radial and tangential components, weighted by surface area
	    area_r = 0.5
	    area_t = 0.5
	    vmac_kernel = area_r*mrkern + area_t*mtkern
	    kernals_to_use.append(vmac_kernel)

    else:
	    pass

    #print('\t- Vmac: {}'.format(time.time()-tic_b))
    tic_b = time.time()
    #########################
    # Construct vsini kernel
    #########################

    if vsini > 0:
        epsilon=0.6
        #wave_ = np.log(wavelength)
        #velo_ = np.linspace(wave_[0],wave_[-1],len(wave_))
        #flux_ = np.interp(lnwave_,wave_,flux)
        #dvelo = velo_[1]-velo_[0]
        dvelo = velocity_step/c_kms
        vrot = vsini/(299792458.*1e-3) # in Km/s
        #-- compute the convolution kernel and normalise it
        # PFLM/SG - changed calculation of kernel to use an odd number of pixels so that 
        # there is at least one pixel in the kernel - more reliable for low vrot
        n = int(1+2*vrot/dvelo) # always have at least one pixel in the kernel !
        #n = int(3+2*vrot/dvelo) # tried at least 3 elements - it fails
        velo_k = np.arange(0,n)*dvelo
        velo_k -= velo_k[-1]/2.
        y = 1 - (velo_k/vrot)**2 # transformation of velocity
        vsini_kernel = (2*(1-epsilon)*np.sqrt(y)+np.pi*epsilon/2.*y)/(np.pi*vrot*(1-epsilon/3.0))  # the kernel
        vsini_kernel /= vsini_kernel.sum()
        kernals_to_use.append(vsini_kernel)
    else:
	    pass

    #vsini_kernel = [1]
    #print('\t- Vsini: {}'.format(time.time()-tic_b))
    tic_b = time.time()
    ##############################
    # Construct resolution kernel
    ##############################
    if resolution > 0:
        #lnwave_ = np.linspace(np.log(wavelength[0]),np.log(wavelength[-1]),len(wavelength))
        #flnw_ = np.interp(lnwave_,np.log(wavelength),flux_spec) # Flux on uniform log wave 
        #dvelo = (np.exp(lnwave_[1]-lnwave_[0])-1)*c_kms
        v_fwhm =c_kms/resolution # in km/s
        n = int(1+4*v_fwhm/velocity_step) # always have at least one pixel in the kernel !
        velo_k = np.linspace(-n*velocity_step,n*velocity_step,2*n + 1)
        v_sigma = v_fwhm/(2*np.sqrt(2*np.log(2)))
        resolution_kernel =  np.exp(-0.5*velo_k**2/(v_sigma**2)) # the kernel
        resolution_kernel /= resolution_kernel.sum()
        kernals_to_use.append(resolution_kernel)
    else:
	    pass
    #print('\t- Resolution: {}'.format(time.time()-tic_b))
    tic_b = time.time()

    #OLD WAY
    '''
    Interpolation: 0.0085289478302
	    - Logspace: 0.00404000282288
	    - Vmac: 0.000119924545288
	    - Vsini: 6.89029693604e-05
	    - Resolution: 7.10487365723e-05
	    - kernel combining: 0.000430107116699
	    - Actual broadening: 0.0108051300049
	    - Interpolating: 0.00414109230042
    Broadening: 0.0197830200195
    Re-interplating: 0.00380778312683
    Dicting: 0.0040590763092
    Total: 0.0362620353699


    '''
    #print('vmac kernel')
    #print(vmac_kernel)

    #print('vsini kernel')
    #print(vsini_kernel)

    #print('resolution kernel')
    #print(resolution_kernel)

    ##########################
    # Create the master kernel
    ##########################
    #print('Number of kernels: {}'.format(len(kernals_to_use)))
    #print(kernals_to_use)

    kernals_to_use = sorted(kernals_to_use, key=len)
    kernals_to_use = kernals_to_use[::-1] # list of kernels - longest first

    master_kernel = kernals_to_use[0]
    for i in range(1,len(kernals_to_use)):
	    master_kernel = signal.convolve(master_kernel,kernals_to_use[i],mode='same')
	
    #master_kernel = signal.convolve(vmac_kernel,vsini_kernel,mode='same')
    #master_kernel = fftconvolve(vsini_kernel,vmac_kernel,mode='same')
    #print(master_kernel)
    #master_kernel = signal.convolve(master_kernel,resolution_kernel,mode='same')
    #master_kernel = fftconvolve(master_kernel,resolution_kernel,mode='same')




    #print(master_kernel)
    #print('\t- kernel combining: {}'.format(time.time()-tic_b))
    tic_b = time.time()
    ###############################
    # Now do the final convolution
    ###############################
    #flnw_conv = 1 - signal.convolve(1-flnw_, master_kernel, mode='same') # Fastest
    flnw_conv = 1 - fftconvolve(1-flnw_, master_kernel, mode='same') # Fastest
    #print('\t- Actual broadening: {}'.format(time.time()-tic_b))
    tic_b = time.time()
    flux_conv = np.interp(wavelength,np.exp(lnwave_),flnw_conv)
    #print('\t- Interpolating: {}'.format(time.time()-tic_b))
    return wavelength, flux_conv
	


def cut_spectrum_from_segments(data,regions_name):
    segments = ispec.read_segment_regions(os.path.join(this_dir, "bad_regions", regions_name))
    wfilter = ispec.create_wavelength_filter(data, regions=segments)
    return data[~wfilter]

def estimate_turbulence_from_doyle(teff,logg):
	microturbulence_vel =  0.89 + (4.16*(teff-5777)*(10**-4))+(9.25*10**(-8))*((teff-5777)**2) # Km/s Amanda Doyles Thesis
	macroturbulence =  3.21 + 2.33*(teff-5777)*(10**-3) + 2.00*(((teff-5777)**2)*10**(-6))-2.00*(logg-4.44) # Km/s Amanda vmic calibration

	#print('microturbulence_vel =  {}, macroturbulence =  {}'.format(microturbulence_vel,macroturbulence))

	return microturbulence_vel, macroturbulence


def lnlike(theta,home, temperature,metalicity,loggs,real,bottom_index,top_index,weight,wavemin,wavemax,n_use,Prior_values,dwt_type,dwt_number,resolution,rtc, path_to_grid,cut_regions,err):
    #tic = time.time()
    teff,mh,logg,vrot,beta = theta
    ##################
    #interpolate model
    ##################
    teff_2_closest = nsmallest(2, list(set(temperature)), key=lambda x: abs(x-teff)); teff_2_closest.sort() #G
    mh_2_closest = nsmallest(2, list(set(metalicity)), key=lambda x: abs(x-mh));mh_2_closest.sort() #G
    logg_2_closest = nsmallest(2, list(set(loggs)), key=lambda x: abs(x-logg));logg_2_closest.sort() #G
    # finding spectra
    models = [[i,j,k] for i in teff_2_closest for j in mh_2_closest for k in logg_2_closest]
    models_index = [get_index(i,temperature,metalicity,loggs) for i in models]
    T,M,L = [i[0] for i in models],[i[1] for i in models],[i[2] for i in models] # temperature coordinates
    # set values to interpolate with and the distance to the box
    t = teff-min(T) # x
    tstep = max(T)-min(T)
    m = mh-min(M) #y
    mstep = max(M)-min(M)
    l=logg-min(L) # z
    lstep=max(L)-min(L)
    models1 = models
    hdulist = fits.open('/home/sam/anaconda3/lib/python3.6/site-packages/waveletspec/Grids/spectrum.fits')
    models =[hdulist[0].data[i] for i in models_index]
    # main interp (Vijk in example)
    interp_spectra = np.array(models[0])*(tstep-t)*(mstep-m)*(lstep-l) \
    + np.array(models[4])*t*(mstep-m)*(lstep-l)\
    + np.array(models[2])*(tstep-t)*m*(lstep-l)\
    + np.array(models[1])*(tstep-t)*(mstep-m)*l\
    + np.array(models[5])*t*(mstep-m)*l \
    +np.array(models[3])*(tstep-t)*m*l \
    + np.array(models[6])*t*m*(lstep-l) \
    + np.array(models[7])*t*l*m
    model =  interp_spectra/(tstep*lstep*mstep)
    model_wave = np.linspace(350,800,2**18)


    ##################
    # Re in-terpolate
    ##################
    f = interp1d(model_wave, model)
    model_wave = np.linspace(wavemin,wavemax,2**n_use)
    model = f(model_wave)

    ###################################
    # Get vmac values from calibration
    ###################################
    '''
    if teff > 6400:
	    macroturbulence = 6
    #elif teff < 5200:
    #	macroturbulence = 2
    else:
	    macroturbulence = 3.21 + 2.33*(teff-5777)*(10**-3) + 2.00*(((teff-5777)**2)*10**(-6))-2.00*(logg-4.44) # Km/s Amanda vmic calibration

    if macroturbulence>6:
	    macroturbulence = 6
    '''
    microturbulence_vel, macroturbulence = estimate_turbulence_from_doyle(teff,logg)
    #macroturbulence = ispec.estimate_vmac(teff,logg,mh) # km/s
    #print(macroturbulence)
    ######################
    # broaden model
    ######################
    #model_wave, model = vmac_broadening(model_wave,model, macroturbulence)
    #model_wave, model = broaden_vsini(model_wave, model, vrot)
    #model_wave, model = instrumentally_broaden(model_wave, model , resolution=resolution)
    model_wave, model = broaden_all_fastn(model_wave,model,macroturbulence,vrot,resolution)
    #plt.plot(data['waveobs'],data['flux'])
    #plt.plot(model_wave, model,'r')
    #plt.show()

    ###################################
    # Now cut line regions if needed
    ###################################
    '''
    if cut_regions != None:
	    # First cut the spectrum
	    data = ispec.create_spectrum_structure(model_wave)
	    data['flux'] = model
	    data = cut_spectrum_from_segments(data, cut_regions)

	    # Now we have to re-interpolate in pixel space so it has 2**n
	    model = np.interp(range(2**n_use),range(len(data)),data['flux'])
	    #plt.plot(model,label='model')
	    #plt.plot(flux,label='flux')
	    #plt.show()
    '''	

    #################################
    # Wavelet transform in log space
    #################################
    model = w.dwt(np.log10(model),dwt_type,dwt_number)

    #######################################
    # To add a log g prior from photometry
    #######################################
    # here is an list of wasp planets and logg
    # wasp	P_prior	P_prior_error
    # 4	4.45	0.029
    # 5 	4.403	0.039
    # 6	4.50	0.06

    # Methodd1 - error matches that of DWT coeffs
    wt = 1./(np.power(err[bottom_index:top_index],2)*beta) # must be real
    #print('SIGMA: {}'.format(sig))
    # method 2
    #wt = 1./(np.power(err[bottom_index:top_index],2) + np.power(sig,2))

    #plt.semilogy(real[bottom_index:top_index],'b')
    #plt.semilogy(model[bottom_index:top_index],'r')
    #plt.show()
    log_like_data = 0.5*(np.sum(np.log(wt)))  -0.5*np.sum(wt*np.power(real[bottom_index:top_index]-model[bottom_index:top_index],2))
    #print('reduceschi_from_data: {}'.format(chi_from_data/len(err[bottom_index:top_index])))
    #########################################
    # Add prior Values (remember - negative) teff,mh,logg,vrot,beta
    #########################################
    chi_T = 0
    if Prior_values[0]!=False and Prior_values[1]!=False:
	    chi_T = np.power(teff-Prior_values[0],2)/np.power(Prior_values[1],2)

    chi_M = 0
    if Prior_values[2]!=False and Prior_values[3]!=False:
	    chi_M = np.power(mh-Prior_values[2],2)/np.power(Prior_values[3],2)

    chi_L = 0
    if Prior_values[4]!=False and Prior_values[5]!=False:
	    chi_L = np.power(logg-Prior_values[4],2)/np.power(Prior_values[5],2)

    chi_V = 0
    if Prior_values[6]!=False and Prior_values[7]!=False:
	    chi_V = np.power(vrot-Prior_values[6],2)/np.power(Prior_values[7],2)

    log_like_priors = -0.5*(chi_T +chi_M + chi_L + chi_V)

    #print('log_lik priors: {}'.format(log_like_priors))
    #print('log like data : {}'.format(log_like_data))
    #print('weight : {}'.format(err))
    # imperfect models atomic data, scales with solar metaliicity. 
    # beta holds some sins.
    ############################
    # Return the log likly hood
    ############################
    #toc = time.time()
    #print('Time: '+str(toc-tic))
    #print(str(chi_from_data + chi_from_priors))
    log_like = log_like_data + log_like_priors
    #print('{:.0f} {:.4f} {:.4f} {:.4f} {:.2f} {:.2f}'.format(teff,mh,logg,vrot,beta,log_like))
    #print('{} {} {} {} {} {}'.format(teff,mh,logg,vrot,sig,log_like))
    return log_like

##############################
# define the lnprior function
##############################	

def lnprior(theta,rtc):
	# teff,mh,logg,vrot,sig = theta
	T,M,L,V,sig  = theta
	if (T<4000) or (T>8000):
		#print('tef')
		return -np.inf
	if (M<-1) or (M>1):
		#print('MH')
		return -np.inf
	#print(rtc)
	if rtc=='spectrum.fits':
		if (L<3.5) or (L>5):
			#print('L')
			return -np.inf
	else:
		print('FAILS')
		return -np.inf
	if (V<0) or (V>100):
		#print('Ve')
		return -np.inf
	if (sig<0):
		#print('sigVe')
		return -np.inf

	#print(T,M,L,V,sig)
	return 0.0

##############################
# define the lnprob function
##############################	

def lnprob(theta,ispec_dir,temperature,metalicity,loggs,real,bottom_index,top_index,weight,wavemin,wavemax,n_use,Prior_values,dwt_type,dwt_number,resolution,rtc, path_to_grid,cut_regions, err):
	lp = lnprior(theta,'spectrum.fits')
	if not np.isfinite(lp):
	    return -np.inf
	return lp + lnlike(theta,ispec_dir,temperature,metalicity,loggs,real,bottom_index,top_index,weight,wavemin,wavemax,n_use,Prior_values,dwt_type,dwt_number,resolution,rtc, path_to_grid,cut_regions,err)


def get_index(i,T,M,L):
	Tindicies = np.where(T==i[0])[0]
	Mindicies = np.where(M==i[1])[0]
	Lindicies = np.where(L==i[2])[0]
	p =  list(set(Tindicies) & set(Mindicies) & set(Lindicies))
	return p[0] 

def wavelet_analysis(data, method = 'grid',wavemin=452,wavemax=648,n_use=17,n_low=4,n_high=14,Nweight=10,T=None,M=None,L=None,V=None,Prior_values='flat',draws=1000,dwt_type=bytes('d', 'utf-8'),dwt_number=4,save_data = True,grid_name='spectrum',resolution=55000,chain_file = 'chain.fits',threads=1,silent=False, cut_regions=None):
    #####################
    # Check within range
    #####################
    if wavemin<min(data['waveobs']) or wavemax>max(data['waveobs']):
	    print('The standard range (452nm-648nm) is out of range of your data.\nYour current range is '+str(min(data['waveobs']))+'nm - '+str(max(data['waveobs']))+'nm\nPlease specify a range with the wavemin and wavemax argument')
	    return

    ###################
    # Set up priors
    ###################
    if Prior_values=='flat':
	    if not silent:
		    print('Using Flat Priors.\n')
	    Prior_values = np.array([False]*10)


    ###################################################
    # Set up log likelihood functions from each method
    ###################################################

    try:
	    rtc = grid_name+'.fits'
	    # path_to_grid = raw_input('Please specify path to grid: ')
	    path_to_grid = os.path.join(this_dir, "Grids", rtc)
	    h = fits.open(path_to_grid)
	    hdulist  =fits.open(path_to_grid)
	    if not silent:
		    print('Using grid from:')
		    print(path_to_grid)
    except:
	    print('Grid not found. change in wavelet_analysis file.')
	    print(path_to_grid)
	    return data




    err = data['err']

    ##############################
    # now set up emcee ensembler #
    ##############################
    ndim = 5 # 4 free parameters
    chains = args.walkers #80 # 2*ndim 
    # ...and a positive definite, non-trivial covariance matrix.
    cov  = 0.5-np.random.rand(ndim**2).reshape((ndim, ndim))
    cov  = np.triu(cov)
    cov += cov.T - np.diag(cov.diagonal())
    cov  = np.dot(cov,cov)
    # Invert the covariance matrix first.
    icov = np.linalg.inv(cov)
    # We'll sample with 250 walkers.





    ###################################
    # Now cut line regions if needed
    ###################################

    # Now we have to re-interpolate in pixel space so it has 2**n
    flux = np.interp(np.linspace(wavemin,wavemax,2**n_use),data['waveobs'],data['flux'])
    fluxe = np.interp(np.linspace(wavemin,wavemax,2**n_use),data['waveobs'],data['err'])

    ###########################################
    # Now get coeffs
    ############################################
    real = w.dwt(np.log10(flux),dwt_type,dwt_number)



    ###########
    # Weighting
    ###########
    if not silent:
	    print('Performing weighting... ',end='')
    weights =np.array([flux for i in range(Nweight)]) # pre-allocate
    if not silent:
	    bar = Bar('Performing weighting',max = Nweight)
    for i in range(Nweight):
	    try:
		    weights[i] = w.dwt(1000 + np.log10(weights[i] + np.random.normal(0,fluxe)),dwt_type,dwt_number)
		    if not silent:
			    bar.next()
	    except:
		    weights[i] = w.dwt(1000 + np.log10(weights[i] + np.random.normal(0,0.1)),dwt_type,dwt_number) # incase no flux err
		    if not silent:
			    bar.next()
    if not silent:
	    bar.finish()

    weight = weights.std(axis=0)
    del weights
    where_are_NaNs = np.isnan(weight)
    where_are_inf = np.isinf(weight)
    weight[where_are_NaNs]=0.003 # botch
    weight[weight<0.0005]=0.0005 # botch dwt_type='d',dwt_number=4
    bottom_index,top_index = (2**n_low) ,(2**n_high) - 1

    ########################
    # Preliminaries for MCMC
    ########################
    Trange,Mrange,Lrange = [],[],[]
    for i in range(0,len(hdulist[1].data)):
	    Trange.append(hdulist[1].data[i][0])
	    Mrange.append(hdulist[1].data[i][1])
	    Lrange.append(hdulist[1].data[i][2])
    Trange,Mrange,Lrange = sorted(list(set(Trange))),sorted(list(set(Mrange))),sorted(list(set(Lrange)))
    step_sizes = [ (Trange[-1]-Trange[0])/(len(Trange)-1),(Mrange[-1]-Mrange[0])/(len(Mrange)-1),(Lrange[-1]-Lrange[0])/(len(Lrange)-1),2] # Step size is originaly grid
    Trange = [min(Trange),max(Trange)]; Mrange = [min(Mrange),max(Mrange)]; Lrange = [min(Lrange),max(Lrange)]; #set inital range as whole grid
    temperature,metalicity,loggs = [i[0] for i in hdulist[1].data],[i[1] for i in hdulist[1].data],[i[2] for i in hdulist[1].data]; #put this outside - shave 0.6 seconds






    # Initialse Walkers roun solar value unless otherwis stated	model=0
    model=0
    sampler,pos=0,0


    sampler = emcee.EnsembleSampler(chains, ndim, lnprob, args = [ispec_dir,temperature,metalicity,loggs,real,bottom_index,top_index,weight,wavemin,wavemax,n_use,Prior_values,dwt_type,dwt_number,resolution,rtc, path_to_grid,cut_regions, err], threads=threads)

    pos = []
    if not silent:
	    bar = Bar('Bulding starting positions',max = chains)
    while len(pos)!= chains:
	    if T == None:
		    T_tmp = np.random.uniform(5500,6000)
		    #T_tmp = np.log10(T_tmp)
	    else:
		    T_tmp =  np.random.normal(T,20)
		    #T_tmp = np.log10(T_tmp)
	    if M == None:
		    M_tmp = np.random.uniform(-0.1,0.1)
	    else:
		    M_tmp = np.random.normal(M,0.01)
	    if L == None:
		    L_tmp = np.random.uniform(4.4,4.5)
	    else:
		    L_tmp = np.random.normal(L,0.01)
	    if V == None:
		    V_tmp = np.random.uniform(5,6)
	    else:
		    V_tmp = np.random.normal(V,0.01)

	    pos_tmp = [T_tmp, M_tmp, L_tmp, V_tmp, np.random.uniform(0,0.03)]
	    #print(pos_tmp)
	    lnlike_trial = lnprob(pos_tmp,ispec_dir,temperature,metalicity,loggs,real,bottom_index,top_index,weight,wavemin,wavemax,n_use,Prior_values,dwt_type,dwt_number,resolution,rtc, path_to_grid,cut_regions, err)
	    #print('Trial function lnlike: {}'.format(lnlike_trial))
	    if lnlike_trial != -np.inf:
		    pos.append(pos_tmp)
		    if not silent:
			    bar.next()
	
    if not silent:
        	bar.finish()
    #pos =np.array([[T+np.random.uniform(0,50),M+np.random.uniform(0,0.001),L+np.random.uniform(0,0.01),V+np.random.uniform(0,1),np.random.uniform(0,15)] for i in range(chains)])


    #################################
    # Run the sampler for a burn in
    #################################
    #pos, prob, state = sampler.run_mcmc(pos,draws)
    #print('done.')

    width=30
    if not silent:
	    start_time = time.time()
	    bar = Progressbar('MCMC', max=draws)
	    print('\n\n')
	    print('    Progrss bar                         Teff          [Fe/H]         Logg           Vsini         e.t.a           Step')
	    print('-----------------------------------------------------------------------------------------------------------------------------')
    for i, result in enumerate(sampler.sample(pos, iterations=draws)):
        #return result
        bar_vals['step'] = result[0]
        n = int((width+1) * float(i) / draws)
        delta_t = time.time()-start_time#  time to do float(i) / n_steps  % of caluculations
        time_incr = delta_t/(float(i+1) / draws) # seconds per increment
        time_left = time_incr*(1- float(i) / draws)
        m, s = divmod(time_left, 60)
        h, m = divmod(m, 60)
        bar_vals['timeleft'] = "{0}h:{1}m:{2}s".format(str(int(h)).zfill(2), str(int(m)).zfill(2), '{:.2f}'.format(s).zfill(2))
        #sys.stdout.write("\r[{0}{1}] {2:.2f}% - {3}h:{4}m:{5:.2f}s\tstep: {6}".format('#' * n, ' ' * (width - n), 100*float(i) / draws ,h, m, s,i))
        if not silent:
        	bar.next()

    if not silent:
        bar.finish()
    #sys.stdout.write("\n")

    ############################################
    # Append the sampler to data for future use
    ############################################
    #data['burn_in_sampler'] = sampler

    ########################
    # Now save to fits file
    ########################

    try:
        os.remove(chain_file)
    except:
        pass

    t = Table(sampler.flatchain, names=['T_eff','MH','Logg','Vsini','beta'])
    t.add_column(Column(sampler.flatlnprobability,name='loglike'))
    indices = np.mgrid[0:chains,0:draws]
    step = indices[1].flatten()

    walker = indices[0].flatten()


    t.add_column(Column(step,name='step'))
    t.add_column(Column(walker,name='walker'))
    if chain_file=='table':
	    return t
    else:
	    t.write(chain_file)


    return data








#
#Print args
#
print('Spectrum file: {}'.format(args.spec_file))
print('Threads: {}'.format(args.threads))
print('Steps: {}'.format(args.steps))
print('Nomalisation: {}'.format(args.normalise_spectrum))
print('Otput file: {}'.format(args.chain_file))

print('Loading: {}'.format(args.spec_file))
data = load_spectra(args.spec_file)

if args.normalise_spectrum=='True':
	print('normalising spectrum')
	data = normalise_spectrum(data)

data['flux'][data['flux'] < 0.001] = 0.001
print('Starting wavelet analysis')
wavelet_analysis(data, draws=args.steps , chain_file= args.chain_file, threads=args.threads)
















